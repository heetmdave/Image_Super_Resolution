{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA CAN RUN 6 CELLS FROM HERE TO SEE OUTPUT . ALSO UPDATE THE IMAGE PATH BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_img_path=\"input.jpg\"\n",
    "\n",
    "# update the custom image path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import vgg19\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = vgg19(pretrained=True).features[:16]  # up to relu_4_1\n",
    "        self.vgg = nn.Sequential(*list(vgg)).eval()\n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, sr, hr):\n",
    "        return F.l1_loss(self.vgg(sr), self.vgg(hr))\n",
    "\n",
    "class TotalVariationLoss(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.mean(torch.abs(x[:, :, :-1] - x[:, :, 1:])) + \\\n",
    "               torch.mean(torch.abs(x[:, :-1, :] - x[:, 1:, :]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + 0.1 * self.block(x)\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels // reduction, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        scale = self.fc(x)\n",
    "        return x * scale\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(2, 1, kernel_size=7, padding=3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        attn = self.conv(torch.cat([max_out, avg_out], dim=1))\n",
    "        return x * attn\n",
    "\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, channels=64, growth=32):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(5):\n",
    "            in_ch = channels + i * growth\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Conv2d(in_ch, growth, 3, padding=1),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            ))\n",
    "        self.final = nn.Conv2d(channels + 5 * growth, channels, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        for layer in self.layers:\n",
    "            features.append(layer(torch.cat(features, dim=1)))\n",
    "        return x + 0.2 * self.final(torch.cat(features, dim=1))\n",
    "\n",
    "class RRDBBlock(nn.Module):\n",
    "    def __init__(self, channels=64):\n",
    "        super().__init__()\n",
    "        self.block1 = DenseBlock(channels)\n",
    "        self.block2 = DenseBlock(channels)\n",
    "        self.block3 = DenseBlock(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + 0.2 * self.block3(self.block2(self.block1(x)))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads=4, mlp_ratio=2.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=heads, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(dim * mlp_ratio), dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x_flat = x.permute(0, 2, 3, 1).reshape(B, H * W, C)\n",
    "        x_norm = self.norm1(x_flat)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n",
    "        x = x_flat + attn_out\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x.reshape(B, H, W, C).permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "class SRModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.res_blocks = nn.Sequential(*[ResidualBlock(64) for _ in range(4)])\n",
    "        self.se = SEBlock(64)\n",
    "        self.sa = SpatialAttention()\n",
    "        self.rrdb = RRDBBlock(64)\n",
    "        self.transformer = TransformerBlock(64)\n",
    "\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(64, 256, 3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(64, 256, 3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(64, 3, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.se(x)\n",
    "        x = self.sa(x)\n",
    "        x = self.rrdb(x)\n",
    "        x = self.transformer(x)\n",
    "        return self.upsample(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SRDatasetFullImage(Dataset):\n",
    "    def __init__(self, hr_dirs, lr_dir):\n",
    "        self.hr_paths = []\n",
    "        for hr_dir in hr_dirs:\n",
    "            self.hr_paths += sorted(glob(os.path.join(hr_dir, \"*.png\")))\n",
    "        \n",
    "        self.lr_paths = sorted(glob(os.path.join(lr_dir, \"*.png\")))\n",
    "        \n",
    "        self.hr_paths = sorted(self.hr_paths, key=lambda x: os.path.basename(x))\n",
    "        self.lr_paths = sorted(self.lr_paths, key=lambda x: os.path.basename(x))\n",
    "\n",
    "        assert len(self.hr_paths) == len(self.lr_paths), \"Mismatch: HR=\" + str(len(self.hr_paths)) + \", LR=\" + str(len(self.lr_paths))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hr_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hr = cv2.imread(self.hr_paths[idx])\n",
    "        lr = cv2.imread(self.lr_paths[idx])\n",
    "\n",
    "        hr = cv2.resize(hr, (384, 384))\n",
    "        lr = cv2.resize(lr, (96, 96))\n",
    "\n",
    "        hr_tensor = torch.FloatTensor(hr / 127.5 - 1.0).permute(2, 0, 1)\n",
    "        lr_tensor = torch.FloatTensor(lr / 127.5 - 1.0).permute(2, 0, 1)\n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "def sr_patchwise_blended(model, lr_image_path, patch_size=96, stride=48, upscale_factor=4, save_name=\"comparison.png\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    img = cv2.imread(lr_image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    # pad image\n",
    "    pad_h = (patch_size - h % patch_size) % patch_size\n",
    "    pad_w = (patch_size - w % patch_size) % patch_size\n",
    "    img_padded = np.pad(img, ((0, pad_h), (0, pad_w), (0, 0)), mode='reflect')\n",
    "    H, W = img_padded.shape[:2]\n",
    "\n",
    "    # accumulators\n",
    "    sr_accum = np.zeros((H * upscale_factor, W * upscale_factor, 3), dtype=np.float32)\n",
    "    weight_mask = np.zeros_like(sr_accum, dtype=np.float32)\n",
    "\n",
    "    # sliding window\n",
    "    for i in range(0, H - patch_size + 1, stride):\n",
    "        for j in range(0, W - patch_size + 1, stride):\n",
    "            patch = img_padded[i:i+patch_size, j:j+patch_size]\n",
    "            patch_tensor = torch.FloatTensor(patch / 127.5 - 1.0).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                sr_patch = model(patch_tensor).clamp(-1, 1)[0]\n",
    "                sr_patch = ((sr_patch.permute(1, 2, 0).cpu().numpy() + 1) * 127.5)\n",
    "\n",
    "            # blend into output canvas\n",
    "            y_start = i * upscale_factor\n",
    "            x_start = j * upscale_factor\n",
    "            y_end = y_start + sr_patch.shape[0]\n",
    "            x_end = x_start + sr_patch.shape[1]\n",
    "\n",
    "            sr_accum[y_start:y_end, x_start:x_end] += sr_patch\n",
    "            weight_mask[y_start:y_end, x_start:x_end] += 1.0\n",
    "\n",
    "    # normalize overlapping regions\n",
    "    sr_output = np.clip(sr_accum / np.maximum(weight_mask, 1e-8), 0, 255).astype(np.uint8)\n",
    "\n",
    "    # crop padding\n",
    "    sr_output = sr_output[:h * upscale_factor, :w * upscale_factor]\n",
    "    lr_upscaled = cv2.resize(img, (sr_output.shape[1], sr_output.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # comparison image\n",
    "    comparison = np.hstack((lr_upscaled, sr_output))\n",
    "    cv2.imwrite(save_name, cv2.cvtColor(comparison, cv2.COLOR_RGB2BGR))\n",
    "    print(\" \" + save_name + \" saved with blended patches.\")\n",
    "\n",
    "\n",
    "    return img, sr_output, lr_upscaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Epoch 43 model on cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = SRModel().to(device)\n",
    "\n",
    "model_path = \"sr_model_epoch_43.pth\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "model.eval()\n",
    "print(\"Loaded Epoch 43 model on\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on 1282 px x 1920 px\n"
     ]
    }
   ],
   "source": [
    "\n",
    "custom_img = cv2.imread(custom_img_path)\n",
    "c_h,c_w=custom_img.shape[:2]\n",
    "print(\"Testing on \"+str(c_h)+\" px x \"+str(c_w)+\" px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR generation STARTED (wait till success message)\n",
      "Resized image saved at: custom_img_degraded_dir/input.jpg\n",
      " comparison.png saved with blended patches.\n",
      "final_sr_output.png saved.\n",
      "SR Success\n"
     ]
    }
   ],
   "source": [
    "custom_img_resized_dir = \"custom_img_degraded_dir\"\n",
    "os.makedirs(custom_img_resized_dir, exist_ok=True)\n",
    "print(\"SR generation STARTED (wait till success message)\")\n",
    "\n",
    "if c_h > 200 or c_w > 200:\n",
    "    target_height = 200\n",
    "    scale = target_height / c_h\n",
    "    target_width = int(c_w * scale)\n",
    "\n",
    "    resized_img = cv2.resize(custom_img, (target_width, target_height), interpolation=cv2.INTER_AREA)\n",
    "    resized_name = os.path.basename(custom_img_path)\n",
    "    custom_img_resized_path = os.path.join(custom_img_resized_dir, resized_name)\n",
    "    cv2.imwrite(custom_img_resized_path, resized_img)\n",
    "    print(\"Resized image saved at: \" + custom_img_resized_path)\n",
    "\n",
    "\n",
    "    sr_input_path = custom_img_resized_path\n",
    "else:\n",
    "    sr_input_path = custom_img_path\n",
    "\n",
    "_, sr_img, _ = sr_patchwise_blended(model, sr_input_path)\n",
    "Image.fromarray(sr_img).save(\"final_sr_output.png\")\n",
    "print(\"final_sr_output.png saved.\")\n",
    "print(\"SR Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA DONT NEED TO RUN BELOW CELLS ELSE THE TRAINING MAY START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BELOW CODE WAS USED DURING TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T11:14:36.391034Z",
     "iopub.status.busy": "2025-04-20T11:14:36.390294Z",
     "iopub.status.idle": "2025-04-20T11:14:36.395226Z",
     "shell.execute_reply": "2025-04-20T11:14:36.394331Z",
     "shell.execute_reply.started": "2025-04-20T11:14:36.391008Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import vgg19\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T11:14:39.957918Z",
     "iopub.status.busy": "2025-04-20T11:14:39.957191Z",
     "iopub.status.idle": "2025-04-20T11:14:39.962701Z",
     "shell.execute_reply": "2025-04-20T11:14:39.961995Z",
     "shell.execute_reply.started": "2025-04-20T11:14:39.957893Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# base dir\n",
    "base_dir = \"/kaggle/working/SuperResolution_Project\"\n",
    "\n",
    "# input hr dir as list\n",
    "div2k_hr_dir = \"/kaggle/input/div2k-dataset/DIV2K_train_HR/DIV2K_train_HR\"\n",
    "flickr2k_hr_dir = \"/kaggle/input/flickr2k-dataset/Flickr2K_HR\"  \n",
    "\n",
    "# combine both for training\n",
    "train_hr_dirs = [div2k_hr_dir, flickr2k_hr_dir]\n",
    "\n",
    "# output folders (where we can write files)\n",
    "train_lr_dir = base_dir + \"/Combined_LR\"\n",
    "progress_dir = base_dir + \"/training_progress\"\n",
    "results_dir = base_dir + \"/results\"\n",
    "model_dir = base_dir + \"/models\"\n",
    "\n",
    "\n",
    "for d in [train_lr_dir, progress_dir, results_dir, model_dir]:\n",
    "    os.makedirs(d, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T11:14:45.738164Z",
     "iopub.status.busy": "2025-04-20T11:14:45.737894Z",
     "iopub.status.idle": "2025-04-20T11:14:45.744386Z",
     "shell.execute_reply": "2025-04-20T11:14:45.743568Z",
     "shell.execute_reply.started": "2025-04-20T11:14:45.738143Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def degrade_image(img):\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    # gaussian blur\n",
    "    k = np.random.choice([5, 7, 9])\n",
    "    sigma = np.random.uniform(0.8, 2.5)\n",
    "    img_blur = cv2.GaussianBlur(img, (k, k), sigma)\n",
    "\n",
    "    # downsample to simulate low-res\n",
    "    img_small = cv2.resize(img_blur, (w // 4, h // 4), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # add Gaussian noise \n",
    "    noise_std = np.random.uniform(3, 15)\n",
    "    noise = np.random.normal(0, noise_std, img_small.shape).astype(np.float32)\n",
    "    img_noisy = np.clip(img_small.astype(np.float32) + noise, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # add haze overlay\n",
    "    haze_value = np.random.randint(180, 230)\n",
    "    haze = np.full_like(img_noisy, haze_value)\n",
    "    alpha = np.random.uniform(0.75, 0.95)\n",
    "    img_hazy = cv2.addWeighted(img_noisy, alpha, haze, 1 - alpha, 0)\n",
    "\n",
    "   \n",
    "    return img_hazy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T11:14:53.766580Z",
     "iopub.status.busy": "2025-04-20T11:14:53.765942Z",
     "iopub.status.idle": "2025-04-20T11:14:53.923482Z",
     "shell.execute_reply": "2025-04-20T11:14:53.922785Z",
     "shell.execute_reply.started": "2025-04-20T11:14:53.766557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total HR images found: 3450\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "div2k_hr_dir = \"/kaggle/input/div2k-dataset/DIV2K_train_HR/DIV2K_train_HR\"\n",
    "flickr2k_hr_dir = \"/kaggle/input/flickr2k/Flickr2K\"  \n",
    "\n",
    "# lr output folder\n",
    "lr_dir = \"/kaggle/working/SuperResolution_Project/Combined_LR\"\n",
    "os.makedirs(lr_dir, exist_ok=True)\n",
    "\n",
    "# get all hr images\n",
    "hr_paths = []\n",
    "for d in [div2k_hr_dir, flickr2k_hr_dir]:\n",
    "    hr_paths += sorted(glob.glob(f\"{d}/*.png\"))\n",
    "\n",
    "print(\"Total HR images found: \" + str(len(hr_paths)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# loop\n",
    "print(\"Total HR images found: \" + str(len(hr_paths)))\n",
    "\n",
    "\n",
    "for hr_path in tqdm(hr_paths, desc=\"Generating LR images\"):\n",
    "    img = cv2.imread(hr_path)\n",
    "    \n",
    "    if img is None:\n",
    "        print(\"Skipped unreadable image: \" + hr_path)\n",
    "\n",
    "        continue\n",
    "\n",
    "    # degrade image \n",
    "    lr_img = degrade_image(img)\n",
    "\n",
    "    # save lr image using same filename\n",
    "    lr_name = os.path.basename(hr_path)\n",
    "    cv2.imwrite(os.path.join(lr_dir, lr_name), lr_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T11:16:05.993273Z",
     "iopub.status.busy": "2025-04-20T11:16:05.993010Z",
     "iopub.status.idle": "2025-04-20T11:16:05.999364Z",
     "shell.execute_reply": "2025-04-20T11:16:05.998646Z",
     "shell.execute_reply.started": "2025-04-20T11:16:05.993253Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models import vgg19\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = vgg19(pretrained=True).features[:16]  # up to relu_4_1\n",
    "        self.vgg = nn.Sequential(*list(vgg)).eval()\n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, sr, hr):\n",
    "        return F.l1_loss(self.vgg(sr), self.vgg(hr))\n",
    "\n",
    "class TotalVariationLoss(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.mean(torch.abs(x[:, :, :-1] - x[:, :, 1:])) + \\\n",
    "               torch.mean(torch.abs(x[:, :-1, :] - x[:, 1:, :]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T11:16:10.433800Z",
     "iopub.status.busy": "2025-04-20T11:16:10.433304Z",
     "iopub.status.idle": "2025-04-20T11:16:10.442417Z",
     "shell.execute_reply": "2025-04-20T11:16:10.441576Z",
     "shell.execute_reply.started": "2025-04-20T11:16:10.433751Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + 0.1 * self.block(x)\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels // reduction, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        scale = self.fc(x)\n",
    "        return x * scale\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(2, 1, kernel_size=7, padding=3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        attn = self.conv(torch.cat([max_out, avg_out], dim=1))\n",
    "        return x * attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T11:16:14.670254Z",
     "iopub.status.busy": "2025-04-20T11:16:14.669685Z",
     "iopub.status.idle": "2025-04-20T11:16:14.676594Z",
     "shell.execute_reply": "2025-04-20T11:16:14.675857Z",
     "shell.execute_reply.started": "2025-04-20T11:16:14.670230Z"
    }
   },
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, channels=64, growth=32):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(5):\n",
    "            in_ch = channels + i * growth\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Conv2d(in_ch, growth, 3, padding=1),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            ))\n",
    "        self.final = nn.Conv2d(channels + 5 * growth, channels, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        for layer in self.layers:\n",
    "            features.append(layer(torch.cat(features, dim=1)))\n",
    "        return x + 0.2 * self.final(torch.cat(features, dim=1))\n",
    "\n",
    "class RRDBBlock(nn.Module):\n",
    "    def __init__(self, channels=64):\n",
    "        super().__init__()\n",
    "        self.block1 = DenseBlock(channels)\n",
    "        self.block2 = DenseBlock(channels)\n",
    "        self.block3 = DenseBlock(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + 0.2 * self.block3(self.block2(self.block1(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T11:16:18.312549Z",
     "iopub.status.busy": "2025-04-20T11:16:18.312279Z",
     "iopub.status.idle": "2025-04-20T11:16:18.318249Z",
     "shell.execute_reply": "2025-04-20T11:16:18.317519Z",
     "shell.execute_reply.started": "2025-04-20T11:16:18.312529Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads=4, mlp_ratio=2.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=heads, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(dim * mlp_ratio), dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x_flat = x.permute(0, 2, 3, 1).reshape(B, H * W, C)\n",
    "        x_norm = self.norm1(x_flat)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n",
    "        x = x_flat + attn_out\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x.reshape(B, H, W, C).permute(0, 3, 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T11:16:22.063268Z",
     "iopub.status.busy": "2025-04-20T11:16:22.062996Z",
     "iopub.status.idle": "2025-04-20T11:16:22.068864Z",
     "shell.execute_reply": "2025-04-20T11:16:22.068254Z",
     "shell.execute_reply.started": "2025-04-20T11:16:22.063251Z"
    }
   },
   "outputs": [],
   "source": [
    "class SRModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.res_blocks = nn.Sequential(*[ResidualBlock(64) for _ in range(4)])\n",
    "        self.se = SEBlock(64)\n",
    "        self.sa = SpatialAttention()\n",
    "        self.rrdb = RRDBBlock(64)\n",
    "        self.transformer = TransformerBlock(64)\n",
    "\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(64, 256, 3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(64, 256, 3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(64, 3, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.se(x)\n",
    "        x = self.sa(x)\n",
    "        x = self.rrdb(x)\n",
    "        x = self.transformer(x)\n",
    "        return self.upsample(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T11:16:26.037945Z",
     "iopub.status.busy": "2025-04-20T11:16:26.037442Z",
     "iopub.status.idle": "2025-04-20T11:16:26.045121Z",
     "shell.execute_reply": "2025-04-20T11:16:26.044324Z",
     "shell.execute_reply.started": "2025-04-20T11:16:26.037922Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SRDatasetFullImage(Dataset):\n",
    "    def __init__(self, hr_dirs, lr_dir):\n",
    "        self.hr_paths = []\n",
    "        for hr_dir in hr_dirs:\n",
    "            self.hr_paths += sorted(glob(os.path.join(hr_dir, \"*.png\")))\n",
    "        \n",
    "        self.lr_paths = sorted(glob(os.path.join(lr_dir, \"*.png\")))\n",
    "        \n",
    "        # ensure hr and lr match by filename\n",
    "        self.hr_paths = sorted(self.hr_paths, key=lambda x: os.path.basename(x))\n",
    "        self.lr_paths = sorted(self.lr_paths, key=lambda x: os.path.basename(x))\n",
    "\n",
    "        assert len(self.hr_paths) == len(self.lr_paths), \"❌ Mismatch: HR=\" + str(len(self.hr_paths)) + \", LR=\" + str(len(self.lr_paths))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hr_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hr = cv2.imread(self.hr_paths[idx])\n",
    "        lr = cv2.imread(self.lr_paths[idx])\n",
    "\n",
    "        # resize\n",
    "        hr = cv2.resize(hr, (384, 384))\n",
    "        lr = cv2.resize(lr, (96, 96))\n",
    "\n",
    "        \n",
    "        hr_tensor = torch.FloatTensor(hr / 127.5 - 1.0).permute(2, 0, 1)\n",
    "        lr_tensor = torch.FloatTensor(lr / 127.5 - 1.0).permute(2, 0, 1)\n",
    "        return lr_tensor, hr_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T11:16:30.817986Z",
     "iopub.status.busy": "2025-04-20T11:16:30.817471Z",
     "iopub.status.idle": "2025-04-20T11:16:30.840714Z",
     "shell.execute_reply": "2025-04-20T11:16:30.840226Z",
     "shell.execute_reply.started": "2025-04-20T11:16:30.817963Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "hr_dirs = [\n",
    "    \"/kaggle/input/div2k-dataset/DIV2K_train_HR/DIV2K_train_HR\",\n",
    "    \"/kaggle/input/flickr2k/Flickr2K\"\n",
    "]\n",
    "\n",
    "lr_dir = \"/kaggle/working/SuperResolution_Project/Combined_LR\"\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    SRDatasetFullImage(hr_dirs, lr_dir),\n",
    "    batch_size=1,  \n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T05:36:03.507848Z",
     "iopub.status.busy": "2025-04-19T05:36:03.507099Z",
     "iopub.status.idle": "2025-04-19T05:36:03.511566Z",
     "shell.execute_reply": "2025-04-19T05:36:03.510852Z",
     "shell.execute_reply.started": "2025-04-19T05:36:03.507823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Om\n"
     ]
    }
   ],
   "source": [
    "print(\"Om\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "log_dir = \"/kaggle/working/training_progress\"\n",
    "os.makedirs(log_dir, exist_ok=True)  \n",
    "\n",
    "with open(os.path.join(log_dir, \"loss_log.txt\"), \"a\") as f:\n",
    "    f.write(\"Logs\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SRModel().to(device)\n",
    "model_path = \"/kaggle/working/models/sr_model_epoch_33.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "print(\" Loaded Epoch 33 model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def lab_color_loss(sr, hr):\n",
    "    # convert [-1, 1] tensors to [0, 255] nump\n",
    "    sr_img = ((sr.detach().cpu().permute(1, 2, 0).numpy() + 1) * 127.5).astype(np.uint8)\n",
    "    hr_img = ((hr.detach().cpu().permute(1, 2, 0).numpy() + 1) * 127.5).astype(np.uint8)\n",
    "\n",
    "    sr_lab = cv2.cvtColor(sr_img, cv2.COLOR_RGB2LAB)\n",
    "    hr_lab = cv2.cvtColor(hr_img, cv2.COLOR_RGB2LAB)\n",
    "\n",
    "    # convert to tensor again for loss\n",
    "    sr_tensor = torch.FloatTensor(sr_lab / 255.).permute(2, 0, 1).to(sr.device)\n",
    "    hr_tensor = torch.FloatTensor(hr_lab / 255.).permute(2, 0, 1).to(hr.device)\n",
    "\n",
    "    return nn.L1Loss()(sr_tensor, hr_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def laplacian_loss(sr, hr):\n",
    "    lap_filter = torch.tensor([[0, -1, 0],\n",
    "                               [-1, 4, -1],\n",
    "                               [0, -1, 0]], dtype=torch.float32, device=sr.device)\n",
    "    lap_filter = lap_filter.view(1, 1, 3, 3).repeat(3, 1, 1, 1)  # shape [3,1,3,3]\n",
    "    \n",
    "    sr_edges = F.conv2d(sr, lap_filter, padding=1, groups=3)\n",
    "    hr_edges = F.conv2d(hr, lap_filter, padding=1, groups=3)\n",
    "    \n",
    "    return F.l1_loss(sr_edges, hr_edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "\n",
    "\n",
    "generator = SRModel().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(generator.parameters(), lr=1e-4)\n",
    "l1_loss = nn.L1Loss()\n",
    "vgg_loss = PerceptualLoss().to(device)\n",
    "tv_loss = TotalVariationLoss().to(device)\n",
    "\n",
    "\n",
    "for epoch in range(34, 101):  \n",
    "    generator.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for lr, hr in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "        lr, hr = lr.to(device), hr.to(device)\n",
    "\n",
    "        sr = generator(lr)\n",
    "\n",
    "        # loss = (\n",
    "        #     0.2 * l1_loss(sr, hr) +\n",
    "        #     0.5 * vgg_loss(sr, hr) +\n",
    "        #     0.1 * tv_loss(sr)\n",
    "        # )\n",
    "        loss = (\n",
    "            1.0 * l1_loss(sr, hr) +\n",
    "            0.5 * vgg_loss(sr, hr) +\n",
    "            0.1 * tv_loss(sr) +\n",
    "            0.2 * lab_color_loss(sr[0], hr[0]) +     \n",
    "            0.05 * laplacian_loss(sr, hr)            \n",
    "        )\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(\"Epoch \" + str(epoch) + \" | Loss: \" + \"{:.4f}\".format(total_loss / len(train_loader)))\n",
    "\n",
    "    \n",
    "    model_path = f\"/kaggle/working/models/sr_model_epoch_{epoch}.pth\"\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    torch.save(generator.state_dict(), model_path)\n",
    "\n",
    "    # save sample sr output for visual inspection\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        lr, hr = train_loader.dataset[0]  \n",
    "        sr = generator(lr.unsqueeze(0).to(device)).clamp(-1, 1)[0]\n",
    "\n",
    "        sr_img = ((sr.permute(1, 2, 0).cpu().numpy() + 1) * 127.5).astype(np.uint8)\n",
    "        hr_img = ((hr.permute(1, 2, 0).cpu().numpy() + 1) * 127.5).astype(np.uint8)\n",
    "        lr_img = ((lr.permute(1, 2, 0).cpu().numpy() + 1) * 127.5).astype(np.uint8)\n",
    "\n",
    "        \n",
    "        lr_up = cv2.resize(lr_img, (sr_img.shape[1], sr_img.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "        comparison = np.hstack([lr_up, sr_img, hr_img])\n",
    "        \n",
    "        psnr_val = psnr(hr_img, sr_img, data_range=255)\n",
    "        ssim_val = ssim(hr_img, sr_img, channel_axis=-1, data_range=255)\n",
    "\n",
    "        \n",
    "        \n",
    "        print(\"Epoch \" + str(epoch) + \" PSNR: \" + \"{:.2f}\".format(psnr_val) + \" | SSIM: \" + \"{:.4f}\".format(ssim_val))\n",
    "\n",
    "       \n",
    "        with open(f\"/kaggle/working/training_progress/loss_log.txt\", \"a\") as f:\n",
    "            f.write(\"Epoch \" + str(epoch) + \" | Loss: \" + \"{:.4f}\".format(total_loss / len(train_loader)) + \n",
    "        \" | PSNR: \" + \"{:.2f}\".format(psnr_val) + \" | SSIM: \" + \"{:.4f}\".format(ssim_val) + \"\\n\")\n",
    "\n",
    "        cv2.imwrite(\"/kaggle/working/epoch_\" + str(epoch) + \"_sample.jpg\", comparison)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below code is fo testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T11:16:49.337788Z",
     "iopub.status.busy": "2025-04-20T11:16:49.337136Z",
     "iopub.status.idle": "2025-04-20T11:16:49.403043Z",
     "shell.execute_reply": "2025-04-20T11:16:49.402179Z",
     "shell.execute_reply.started": "2025-04-20T11:16:49.337764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Epoch 43 model on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3788568203.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = SRModel().to(device)\n",
    "\n",
    "model_path = \"/kaggle/working/models/sr_model_epoch_43.pth\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "model.eval()\n",
    "print(\"Loaded Epoch 43 model on\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T11:16:53.896172Z",
     "iopub.status.busy": "2025-04-20T11:16:53.895935Z",
     "iopub.status.idle": "2025-04-20T11:16:54.492063Z",
     "shell.execute_reply": "2025-04-20T11:16:54.491509Z",
     "shell.execute_reply.started": "2025-04-20T11:16:53.896156Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "\n",
    "set14_dir = \"/kaggle/input/set-5-14-super-resolution-dataset/Set14/Set14\"\n",
    "# set14_dir = \"/kaggle/input/heetdave2\"\n",
    "output_dir = \"/kaggle/working/results_epoch43_set14\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "image_paths = sorted(glob.glob(os.path.join(set14_dir, \"*.png\")) + glob.glob(os.path.join(set14_dir, \"*.jpg\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_resolve_image(img_path, model, upscale=4):\n",
    "    img = cv2.imread(img_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    h, w = img_rgb.shape[:2]\n",
    "    lr = cv2.resize(img_rgb, (w // upscale, h // upscale), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    # normalize [-1, 1]\n",
    "    lr_tensor = torch.tensor(lr / 127.5 - 1.0, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sr_tensor = model(lr_tensor).clamp(-1, 1)[0]\n",
    "\n",
    "    sr_img = ((sr_tensor.permute(1, 2, 0).cpu().numpy() + 1) * 127.5).astype(np.uint8)\n",
    "    return lr, sr_img, img_rgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Om\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "# shutil.rmtree(\"/kaggle/working/results_epoch9_set14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "\n",
    "# def sr_patchwise(model, lr_image_path, patch_size=96, upscale_factor=4, min_display_width=600):\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model = model.to(device).eval()\n",
    "\n",
    "#     # load and prepare lr image\n",
    "#     img = cv2.imread(lr_image_path)\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#     h, w = img.shape[:2]\n",
    "\n",
    "#     # pad if needed\n",
    "#     pad_h = (patch_size - h % patch_size) % patch_size\n",
    "#     pad_w = (patch_size - w % patch_size) % patch_size\n",
    "#     img_padded = np.pad(img, ((0, pad_h), (0, pad_w), (0, 0)), mode='constant', constant_values=0)\n",
    "#     H, W = img_padded.shape[:2]\n",
    "\n",
    "#     # prepare empty sr output image\n",
    "#     sr_output = np.zeros((H * upscale_factor, W * upscale_factor, 3), dtype=np.uint8)\n",
    "\n",
    "#     # process each patch\n",
    "#     for i in range(0, H, patch_size):\n",
    "#         for j in range(0, W, patch_size):\n",
    "#             patch = img_padded[i:i+patch_size, j:j+patch_size]\n",
    "#             patch_tensor = torch.FloatTensor(patch / 127.5 - 1.0).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 sr_patch = model(patch_tensor).clamp(-1, 1)[0]\n",
    "#                 sr_patch = ((sr_patch.permute(1, 2, 0).cpu().numpy() + 1) * 127.5).astype(np.uint8)\n",
    "\n",
    "#             sr_output[i*upscale_factor:(i+patch_size)*upscale_factor,\n",
    "#                       j*upscale_factor:(j+patch_size)*upscale_factor] = sr_patch\n",
    "\n",
    "#     # crop out padding\n",
    "#     final_sr = sr_output[:h*upscale_factor, :w*upscale_factor]\n",
    "    \n",
    "\n",
    "#     # create comparision\n",
    "#     lr_upscaled = cv2.resize(img, (final_sr.shape[1], final_sr.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "#     comparison = np.hstack((lr_upscaled, final_sr))\n",
    "\n",
    "#     # upscale whole comparison if it's visually too small\n",
    "#     if comparison.shape[1] < min_display_width:\n",
    "#         scale_factor = int(np.ceil(min_display_width / comparison.shape[1]))\n",
    "#         comparison = cv2.resize(comparison, (comparison.shape[1]*scale_factor, comparison.shape[0]*scale_factor),\n",
    "#                                 interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "#     # save\n",
    "#     cv2.imwrite(\"comparison.png\", cv2.cvtColor(comparison, cv2.COLOR_RGB2BGR))\n",
    "#     print(\"comparison.png saved.\")\n",
    "\n",
    "#     return img, final_sr, lr_upscaled\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T05:36:28.148488Z",
     "iopub.status.busy": "2025-04-19T05:36:28.147913Z",
     "iopub.status.idle": "2025-04-19T05:36:28.152669Z",
     "shell.execute_reply": "2025-04-19T05:36:28.151847Z",
     "shell.execute_reply.started": "2025-04-19T05:36:28.148451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n"
     ]
    }
   ],
   "source": [
    "print(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T11:17:10.531726Z",
     "iopub.status.busy": "2025-04-20T11:17:10.531348Z",
     "iopub.status.idle": "2025-04-20T11:17:10.542688Z",
     "shell.execute_reply": "2025-04-20T11:17:10.542010Z",
     "shell.execute_reply.started": "2025-04-20T11:17:10.531702Z"
    }
   },
   "outputs": [],
   "source": [
    "def sr_patchwise_blended(model, lr_image_path, patch_size=96, stride=48, upscale_factor=4, save_name=\"comparison.png\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    # load lr image\n",
    "    img = cv2.imread(lr_image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    # pad image to fit patches\n",
    "    pad_h = (patch_size - h % patch_size) % patch_size\n",
    "    pad_w = (patch_size - w % patch_size) % patch_size\n",
    "    img_padded = np.pad(img, ((0, pad_h), (0, pad_w), (0, 0)), mode='reflect')\n",
    "    H, W = img_padded.shape[:2]\n",
    "\n",
    "    # prepare accumulators\n",
    "    sr_accum = np.zeros((H * upscale_factor, W * upscale_factor, 3), dtype=np.float32)\n",
    "    weight_mask = np.zeros_like(sr_accum, dtype=np.float32)\n",
    "\n",
    "    # sliding window\n",
    "    for i in range(0, H - patch_size + 1, stride):\n",
    "        for j in range(0, W - patch_size + 1, stride):\n",
    "            patch = img_padded[i:i+patch_size, j:j+patch_size]\n",
    "            patch_tensor = torch.FloatTensor(patch / 127.5 - 1.0).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                sr_patch = model(patch_tensor).clamp(-1, 1)[0]\n",
    "                sr_patch = ((sr_patch.permute(1, 2, 0).cpu().numpy() + 1) * 127.5)\n",
    "\n",
    "            # blend into output canvas\n",
    "            y_start = i * upscale_factor\n",
    "            x_start = j * upscale_factor\n",
    "            y_end = y_start + sr_patch.shape[0]\n",
    "            x_end = x_start + sr_patch.shape[1]\n",
    "\n",
    "            sr_accum[y_start:y_end, x_start:x_end] += sr_patch\n",
    "            weight_mask[y_start:y_end, x_start:x_end] += 1.0\n",
    "\n",
    "    # normalize overlapping regions\n",
    "    sr_output = np.clip(sr_accum / np.maximum(weight_mask, 1e-8), 0, 255).astype(np.uint8)\n",
    "\n",
    "    # crop padding\n",
    "    sr_output = sr_output[:h * upscale_factor, :w * upscale_factor]\n",
    "    lr_upscaled = cv2.resize(img, (sr_output.shape[1], sr_output.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # comparison image\n",
    "    comparison = np.hstack((lr_upscaled, sr_output))\n",
    "    cv2.imwrite(save_name, cv2.cvtColor(comparison, cv2.COLOR_RGB2BGR))\n",
    "    print(\" \" + save_name + \" saved with blended patches.\")\n",
    "\n",
    "\n",
    "    return img, sr_output, lr_upscaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T11:19:59.701248Z",
     "iopub.status.busy": "2025-04-20T11:19:59.700699Z",
     "iopub.status.idle": "2025-04-20T11:20:05.074587Z",
     "shell.execute_reply": "2025-04-20T11:20:05.073695Z",
     "shell.execute_reply.started": "2025-04-20T11:19:59.701224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ comparison.png saved with blended patches.\n",
      "✅ Saved /kaggle/working/results_epoch43_set14/baboon_compare.jpg\n",
      "📊 baboon | PSNR: 19.51 | SSIM: 0.3490\n",
      "✅ comparison.png saved with blended patches.\n",
      "✅ Saved /kaggle/working/results_epoch43_set14/barbara_compare.jpg\n",
      "📊 barbara | PSNR: 22.35 | SSIM: 0.5534\n",
      "✅ comparison.png saved with blended patches.\n",
      "✅ Saved /kaggle/working/results_epoch43_set14/bridge_compare.jpg\n",
      "📊 bridge | PSNR: 22.13 | SSIM: 0.4798\n",
      "✅ comparison.png saved with blended patches.\n",
      "✅ Saved /kaggle/working/results_epoch43_set14/coastguard_compare.jpg\n",
      "📊 coastguard | PSNR: 22.59 | SSIM: 0.4326\n",
      "✅ comparison.png saved with blended patches.\n",
      "✅ Saved /kaggle/working/results_epoch43_set14/comic_compare.jpg\n",
      "📊 comic | PSNR: 20.78 | SSIM: 0.5986\n",
      "✅ comparison.png saved with blended patches.\n",
      "✅ Saved /kaggle/working/results_epoch43_set14/face_compare.jpg\n",
      "📊 face | PSNR: 25.93 | SSIM: 0.5320\n",
      "✅ comparison.png saved with blended patches.\n",
      "✅ Saved /kaggle/working/results_epoch43_set14/flowers_compare.jpg\n",
      "📊 flowers | PSNR: 23.46 | SSIM: 0.6901\n",
      "✅ comparison.png saved with blended patches.\n",
      "✅ Saved /kaggle/working/results_epoch43_set14/foreman_compare.jpg\n",
      "📊 foreman | PSNR: 24.75 | SSIM: 0.7725\n",
      "✅ comparison.png saved with blended patches.\n",
      "✅ Saved /kaggle/working/results_epoch43_set14/lenna_compare.jpg\n",
      "📊 lenna | PSNR: 26.95 | SSIM: 0.7013\n",
      "✅ comparison.png saved with blended patches.\n",
      "✅ Saved /kaggle/working/results_epoch43_set14/man_compare.jpg\n",
      "📊 man | PSNR: 23.36 | SSIM: 0.5798\n",
      "✅ comparison.png saved with blended patches.\n",
      "✅ Saved /kaggle/working/results_epoch43_set14/monarch_compare.jpg\n",
      "📊 monarch | PSNR: 23.13 | SSIM: 0.6554\n",
      "✅ comparison.png saved with blended patches.\n",
      "✅ Saved /kaggle/working/results_epoch43_set14/pepper_compare.jpg\n",
      "📊 pepper | PSNR: 24.38 | SSIM: 0.6202\n",
      "✅ comparison.png saved with blended patches.\n",
      "✅ Saved /kaggle/working/results_epoch43_set14/ppt3_compare.jpg\n",
      "📊 ppt3 | PSNR: 19.94 | SSIM: 0.7691\n",
      "✅ comparison.png saved with blended patches.\n",
      "✅ Saved /kaggle/working/results_epoch43_set14/zebra_compare.jpg\n",
      "📊 zebra | PSNR: 22.36 | SSIM: 0.6148\n"
     ]
    }
   ],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "for img_path in image_paths:\n",
    "    name = os.path.basename(img_path).split('.')[0]\n",
    "\n",
    "    # load hr image\n",
    "    hr_img = cv2.imread(img_path)\n",
    "    hr_img = cv2.cvtColor(hr_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # downsample\n",
    "    h, w = hr_img.shape[:2]\n",
    "    #lr_img = cv2.resize(hr_img, (w // 4, h // 4), interpolation=cv2.INTER_CUBIC)\n",
    "    lr_img=degrade_image(hr_img)\n",
    "    # save temp lr\n",
    "    temp_lr_path = f\"/kaggle/working/temp_lr/{name}_lr.png\"\n",
    "    os.makedirs(os.path.dirname(temp_lr_path), exist_ok=True)\n",
    "    cv2.imwrite(temp_lr_path, cv2.cvtColor(lr_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # run\n",
    "    lr, sr, _ = sr_patchwise_blended(model, temp_lr_path)\n",
    "    # lr, sr, _ = super_resolve_image(temp_lr_path,model)\n",
    "\n",
    "    # resize\n",
    "    hr_resized = cv2.resize(hr_img, sr.shape[:2][::-1])\n",
    "    min_h = min(lr.shape[0] * 4, sr.shape[0], hr_resized.shape[0])\n",
    "\n",
    "    # stacking\n",
    "    sr_resized = cv2.resize(sr, (int(sr.shape[1] * min_h / sr.shape[0]), min_h))\n",
    "    lr_up = cv2.resize(lr, (sr_resized.shape[1], min_h), interpolation=cv2.INTER_NEAREST)\n",
    "    hr_resized = cv2.resize(hr_resized, (sr_resized.shape[1], min_h))\n",
    "\n",
    "    # save\n",
    "    comp = np.hstack([lr_up, sr_resized, hr_resized])\n",
    "    save_path = os.path.join(output_dir, f\"{name}_compare.jpg\")\n",
    "    cv2.imwrite(save_path, cv2.cvtColor(comp, cv2.COLOR_RGB2BGR))\n",
    "    print(\"Saved \" + save_path)\n",
    "\n",
    "\n",
    "    # === PSNR / SSIM ===\n",
    "    hr_crop = cv2.resize(hr_img, sr.shape[:2][::-1])\n",
    "    psnr = peak_signal_noise_ratio(hr_crop, sr, data_range=255)\n",
    "    ssim = structural_similarity(hr_crop, sr, channel_axis=-1, data_range=255)\n",
    "    print(\" \" + name + \" | PSNR: \" + \"{:.2f}\".format(psnr) + \" | SSIM: \" + \"{:.4f}\".format(ssim))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r /kaggle/working/results_epoch43_set14.zip /kaggle/working/results_epoch43_set14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_img_path=\"/kaggle/input/customimages/trendy-colour-swatches-collage.jpg\"\n",
    "custom_img = cv2.imread(custom_img_path)\n",
    "c_h,c_w=custom_img.shape[:2]\n",
    "print(\"Testing on \"+str(c_h)+\" px x \"+str(c_w)+\" px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_img_resized_dir = \"/kaggle/working/custom_img_degraded_dir\"\n",
    "os.makedirs(custom_img_resized_dir, exist_ok=True)\n",
    "if c_h > 200 or c_w > 200:\n",
    "    target_height = 200\n",
    "    scale = target_height / c_h\n",
    "    target_width = int(c_w * scale)\n",
    "\n",
    "    resized_img = cv2.resize(custom_img, (target_width, target_height), interpolation=cv2.INTER_AREA)\n",
    "    resized_name = os.path.basename(custom_img_path)\n",
    "    custom_img_resized_path = os.path.join(custom_img_resized_dir, resized_name)\n",
    "    cv2.imwrite(custom_img_resized_path, resized_img)\n",
    "    print(\"Resized image saved at: \" + custom_img_resized_path)\n",
    "\n",
    "\n",
    "    sr_input_path = custom_img_resized_path\n",
    "else:\n",
    "    sr_input_path = custom_img_path\n",
    "\n",
    "_, sr_img, _ = sr_patchwise_blended(model, sr_input_path)\n",
    "Image.fromarray(sr_img).save(\"final_sr_output.png\")\n",
    "print(\"final_sr_output.png saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# cpu\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# load\n",
    "model = SRModel().to(device)\n",
    "model.load_state_dict(torch.load(\"/kaggle/working/models/sr_model_epoch_43.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "#sr_img = sr_patchwise_blended(model, custom_img_degraded_path)\n",
    "\n",
    "if c_h<=200 or c_w<=200:\n",
    "    _, sr_img, _ = sr_patchwise_blended(model, custom_img_path)\n",
    "    Image.fromarray(sr_img).save(\"final_sr_output.png\")\n",
    "    print(\"Saved final_sr_output.png\")\n",
    "else:\n",
    "    _, sr_img, _ = sr_patchwise_blended(model, custom_img_degraded_path)\n",
    "    Image.fromarray(sr_img).save(\"final_sr_output.png\")\n",
    "    print(\"Saved final_sr_output.png\")\n",
    "    custom_hr_resized = cv2.resize(custom_img, (sr_img.shape[1], sr_img.shape[0]))\n",
    "    \n",
    "    psnr = peak_signal_noise_ratio(custom_hr_resized, sr_img, data_range=255)\n",
    "    ssim = structural_similarity(custom_hr_resized, sr_img, channel_axis=2, data_range=255)\n",
    "    \n",
    "    print(\"PSNR: \"+str(psnr)+\"dB\")\n",
    "    print(\"SSIM: \"+str(ssim))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T05:36:56.716927Z",
     "iopub.status.busy": "2025-04-19T05:36:56.716627Z",
     "iopub.status.idle": "2025-04-19T05:36:56.720842Z",
     "shell.execute_reply": "2025-04-19T05:36:56.719987Z",
     "shell.execute_reply.started": "2025-04-19T05:36:56.716906Z"
    }
   },
   "outputs": [],
   "source": [
    "GIF_PATH = \"/kaggle/input/customimages/zebra5.gif\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T05:37:04.166002Z",
     "iopub.status.busy": "2025-04-19T05:37:04.165725Z",
     "iopub.status.idle": "2025-04-19T05:37:04.170843Z",
     "shell.execute_reply": "2025-04-19T05:37:04.170132Z",
     "shell.execute_reply.started": "2025-04-19T05:37:04.165981Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"gif_extracted_frames\", exist_ok=True)\n",
    "os.makedirs(\"gif_sr_frames\", exist_ok=True)\n",
    "os.makedirs(\"gif_sr_output\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T05:43:50.138791Z",
     "iopub.status.busy": "2025-04-19T05:43:50.138009Z",
     "iopub.status.idle": "2025-04-19T05:43:50.150180Z",
     "shell.execute_reply": "2025-04-19T05:43:50.149377Z",
     "shell.execute_reply.started": "2025-04-19T05:43:50.138765Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Collected frame durations: [50, 50, 50, 50, 50] ...\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "gif = Image.open(GIF_PATH)\n",
    "durations = []\n",
    "\n",
    "for i in range(gif.n_frames):\n",
    "    gif.seek(i)\n",
    "    duration = gif.info.get(\"duration\", 80)  \n",
    "    durations.append(duration)\n",
    "\n",
    "print(\"Collected frame durations:\", durations[:5], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T05:43:52.365634Z",
     "iopub.status.busy": "2025-04-19T05:43:52.365355Z",
     "iopub.status.idle": "2025-04-19T05:43:56.604209Z",
     "shell.execute_reply": "2025-04-19T05:43:56.603370Z",
     "shell.execute_reply.started": "2025-04-19T05:43:52.365615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ comparison.png saved with blended patches.\n",
      "✅ comparison.png saved with blended patches.\n",
      "✅ comparison.png saved with blended patches.\n",
      "✅ comparison.png saved with blended patches.\n",
      "✅ comparison.png saved with blended patches.\n",
      "✅ comparison.png saved with blended patches.\n",
      "✅ comparison.png saved with blended patches.\n",
      "✅ comparison.png saved with blended patches.\n",
      "✅ comparison.png saved with blended patches.\n",
      "✅ comparison.png saved with blended patches.\n",
      "✅ Super-resolution applied to all frames\n"
     ]
    }
   ],
   "source": [
    "for path in extracted_paths:\n",
    "    _, sr_img, _ = sr_patchwise_blended(model, path)\n",
    "    out_path = path.replace(\"gif_extracted_frames\", \"gif_sr_frames\")\n",
    "    Image.fromarray(sr_img).save(out_path)\n",
    "\n",
    "print(\"Super-resolution applied to all frames\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T05:43:59.150472Z",
     "iopub.status.busy": "2025-04-19T05:43:59.149919Z",
     "iopub.status.idle": "2025-04-19T05:44:00.258800Z",
     "shell.execute_reply": "2025-04-19T05:44:00.258039Z",
     "shell.execute_reply.started": "2025-04-19T05:43:59.150446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SR GIF saved with original speed: gif_sr_output/final_sr_output.gif\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "sr_frame_paths = sorted([f\"gif_sr_frames/{f}\" for f in os.listdir(\"gif_sr_frames\") if f.endswith(\".png\")])\n",
    "sr_images = [Image.open(f).convert(\"RGB\") for f in sr_frame_paths]\n",
    "\n",
    "durations = durations[:len(sr_images)]\n",
    "\n",
    "output_gif_path = \"gif_sr_output/final_sr_output.gif\"\n",
    "sr_images[0].save(\n",
    "    output_gif_path,\n",
    "    save_all=True,\n",
    "    append_images=sr_images[1:],\n",
    "    duration=durations,  # Use original frame durations\n",
    "    loop=0\n",
    ")\n",
    "\n",
    "print(\"✅ SR GIF saved with original speed: \" + output_gif_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 271102,
     "sourceId": 562453,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 286056,
     "sourceId": 588358,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2850129,
     "sourceId": 4914529,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7184919,
     "sourceId": 11469245,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
